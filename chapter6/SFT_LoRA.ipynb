{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ZufpwnkMg7",
        "outputId": "27b1f7a2-eb56-4641-cee8-8609e5ff8bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.12.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.12.2-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 trl-0.12.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XshoufMhkKF0"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# from trl import SFTConfig, SFTTrainer\n",
        "# from peft import LoraConfig\n",
        "\n",
        "# dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "\n",
        "# peft_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "# )\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     \"EleutherAI/gpt-neo-125m\",\n",
        "#     train_dataset=dataset,\n",
        "#     args=SFTConfig(output_dir=\"/tmp\",report_to= 'none'),\n",
        "#     peft_config=peft_config,\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C0Ii9yLyCMrp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "16d398d5d7614a0080e6fb1f75e6a322",
            "9edc13679a824ad58a4fc99895603702",
            "a28e56cfecda4fc59420e6c6f04c974a",
            "b71ba1dc83004d02b2ead663678980e0",
            "02e5b379d69948f7a8f2bf9e9eae2240",
            "43e422cf1f8941ba93370c27b8153693",
            "ac527bf7ace740a69f4e49544a45f480",
            "da61929a85d94222b1f1d364c717f035",
            "880948a409cb4ab78020a660ab0e0bb4",
            "8b350dffe8d847e5bd0101217b10b934",
            "ab98bd96a3a547af8e25536219325507",
            "0a25374a6e35467188fc0282b6ada7d0",
            "89bb2fd36f934bb4bf6884c36da11342",
            "54a1d73c687b4d60b28b1c8fa6915d21",
            "a7ab2b7674414feaac9f8ed831dd0b99",
            "c36cb4def92b4e09b0cd05f9b6d68498",
            "1af3d84515b142bbab89241831390253",
            "4015ecede8434b978411f2210d5f157a",
            "d7090272daa546c6b4a657a3a1438b50",
            "a4553921424e4efeb266e6fdcb6d0b82",
            "6ee64fe4929b4337a0ab9a5a6cf4efb9",
            "e731c5aad1024a899f9576add2fc5dfb",
            "58b412984a4e4c51b1a93f75ae1f0dd5",
            "d8e1d533de1c4f96a5aca4662af6e2c8",
            "f294e30152e14bf1bc72e544f0eaf274",
            "d1b895d7827a4eef901279393497f3f3",
            "d6c39657aa614c12984228463169f7f6",
            "e5876c0a26ac4f0d82d9e99e4c023aeb",
            "49ee01685efe4c5ea8f8744d6cb167d0",
            "dbfcee6469cc4285803c6fea0586d6fc",
            "283f4e43aca64de29887c5a29a262b5e",
            "f1886e15d54a43bf9912f2d0904ad8f0",
            "6401a9b5a54e452b9cf4bfb13a6b8e33",
            "81c2a67a33314af2842a5567cb84aebf",
            "acbe880c11744448994c61c8a3db22b7",
            "faf73f632fc74bf2ac22d562c6272383",
            "3880135bc6bc4ade87536ccaf8a99e9c",
            "163a01b9e14a47cb9f57e7d1a1e0f8a6",
            "3e7b0d993e4647efa43fd320c238f422",
            "817307170e424c16b1b83e558495738e",
            "4c78b223e3204dfcad84e399e7cb144c",
            "5ac1410181b340fd8f1662eed3a91c0a",
            "2776c9654c9c442793e23e055476d172",
            "41967dde550b4995875eef358b21ea8b",
            "3956244f75bb411fa0190781534fef2e",
            "08bc51f26809479cb543c3167e700f14",
            "f1706c20fccb4611a268a916568f1758",
            "f40718a6ffd9489ab81af81dae5955b7",
            "1dc5d1821bfb4834a804e24c2afa4c7b",
            "687b8802f0144717bb03efd2c24021d9",
            "59a93e145da94fd4a3707e5423db5e25",
            "f42f2a5f18f84ef8aba205d947547cf9",
            "b07fd098edc84f6fa4e9f12498d3d80a",
            "a309a30315694100a2dbcff4bf396781",
            "662673f0c6514853ad4ac7682a70633f",
            "5bcafa9ff2c44a8ca31bd401294b496e",
            "544fdf56ee8b47a2bc4511d125da1e37",
            "2e80d71f9c0e49cabb31560772ed4530",
            "6ec28338090644758a3567eb8bc976f2",
            "1ba3723e09024e3394a87a5448eb9c6a",
            "01da23975e7145eea7329fc727dd8bc5",
            "e7a7123467c043358a6586704aad2663",
            "18ba1d3b6b64431d924ecf49276e14a7",
            "b82571bf06b945d7a1520feb44793fa1",
            "737ccc1cb5c44b14aa20d2dd322060d8",
            "37b82e6c93b8407db46b8f5dd8994f24",
            "643426ee7587420fb989663ecc16ec51",
            "44fe2cee10fa4356a94c3618d6a2eec5",
            "1092f57e79ef4998bdd3414d20c34b8d",
            "1b23160ae49d4e82a241850eb1254651",
            "356a9eddbacf44a18385ed83984b5f62",
            "0ec7325dd10b4a4c8ba39c0207314fbe",
            "276aa83c8fd14368ac97fd996051b10c",
            "62d35f1b2d624fb8982a26a1dc8a3f3f",
            "d5a696de4d114814b10ec88f741592c0",
            "6438d6c953a24f3bbd3a939f5d114166",
            "729086410b514c278589943935d58782"
          ]
        },
        "id": "hLUQyX3nCYgz",
        "outputId": "0ec66951-0553-4036-b049-b664f3530717"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16d398d5d7614a0080e6fb1f75e6a322",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a25374a6e35467188fc0282b6ada7d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58b412984a4e4c51b1a93f75ae1f0dd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81c2a67a33314af2842a5567cb84aebf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3956244f75bb411fa0190781534fef2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bcafa9ff2c44a8ca31bd401294b496e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "643426ee7587420fb989663ecc16ec51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 1. Load Pre-trained Model and Tokenizer\n",
        "model_name = \"gpt2\"  # Can be any Hugging Face causal language model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Set a padding token for GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-aC2AxUpbtn",
        "outputId": "3b2d9a3a-b984-43c2-cd4c-21488efc4f24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnz2fNHUCh6g",
        "outputId": "fc7554fe-13b4-4f6c-8070-38d4fe8f3982"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 2. Define PEFT Configuration (LoRA)\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,          # Rank of LoRA matrices\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\"],  # Adjusted for GPT-2\n",
        ")\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUwIBYr-EWsj",
        "outputId": "866092af-cadd-4776-df78-b3b0a3b25931"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2SdpaAttention(\n",
              "              (c_attn): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=2304, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (c_proj): Conv1D(nf=768, nx=768)\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=3072)\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7DSNbChYDeNg"
      },
      "outputs": [],
      "source": [
        "# 3. Prepare Custom Dataset\n",
        "data_1 =  {\n",
        "    \"text\": [\n",
        "        \"Artificial intelligence is revolutionizing many industries by automating tasks and improving decision-making.\",\n",
        "        \"Machine learning models are used in applications like predictive analytics and natural language processing.\",\n",
        "        \"AI can assist in various sectors such as healthcare, finance, and transportation by analyzing large datasets.\",\n",
        "        \"Deep learning techniques are powering advancements in computer vision, enabling machines to interpret images and video.\",\n",
        "        \"AI technologies like robotics are being deployed to perform tasks that were once considered too complex for machines.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset_1 = Dataset.from_dict(data_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BT-wZDEly6xz"
      },
      "outputs": [],
      "source": [
        "data_2 = {\n",
        "    \"text\": [\n",
        "        \"AI is transforming oncology by helping in the early detection of cancer through advanced imaging techniques.\",\n",
        "        \"Machine learning algorithms are increasingly used to analyze tumor biopsies and predict cancer progression.\",\n",
        "        \"In precision oncology, AI is utilized to create personalized treatment plans based on a patient's genetic profile.\",\n",
        "        \"AI-driven systems are improving the accuracy of radiology by identifying tumors in medical scans like CT and MRI images.\",\n",
        "        \"Artificial intelligence is accelerating drug discovery, identifying potential cancer therapies faster than traditional methods.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset_2 = Dataset.from_dict(data_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0fyVsJpAzATp"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    tokenized = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Add labels for loss computation\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5iW9bhnIEDVq"
      },
      "outputs": [],
      "source": [
        "# 4. Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=3,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"no\",  # Skip saving for simplicity\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    report_to= 'none',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3140c6ac3aa04802b8d9ddc2e0ef74f3",
            "54a31a0f55464fda8d0829fa1f6bd93f",
            "4e52cd27238e41428ffd32f313ddf920",
            "2199a7954cf64f88b90e79917823fd5b",
            "907997a0a73d4e189a1825f38da380cf",
            "2b6afe27fa1e45c2a44e8080bbebdd87",
            "205b4cbd7e8a4f77b03d368c1ae85724",
            "7de91928165b4fdab2267ae4cda406f8",
            "d056773dc661444baac552276bf3106e",
            "b6a9ced834ed4d9f9face6f706ab22dd",
            "9d820a503e2b46eaaa4918d67d5c6063",
            "f2f415265cb143328634345f7703361f",
            "d5f7668bad1e4db4bc083c35593e5cf7",
            "e33a60e096ef4dfbbebc66cc1bc71a08",
            "4ace9c0b18ec432cb0b1ab330feb3637",
            "01cb75a5dd954fbe95cbd0c0efc19282",
            "84fb48d948144834aa8d51fb88ac7c7a",
            "4572e8fa13074bb599a7c7038c74ba96",
            "cc287067dc4543ea962112ac6012d143",
            "6ae97b9642e74edd9b9c6aa607050bca",
            "8e05ff7356b84303b7bb5c8074d2aa39",
            "0e7b766b1b6b46fab6cb71b4709a9bb5"
          ]
        },
        "id": "wCkT5sl1TQmD",
        "outputId": "99439acf-e07c-4ef1-fccd-dac2b83713a3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3140c6ac3aa04802b8d9ddc2e0ef74f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2f415265cb143328634345f7703361f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset1 = dataset_1.map(tokenize_function, batched=True)\n",
        "tokenized_dataset2 = dataset_2.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMFoUqKTfvy",
        "outputId": "21bc5881-e836-420e-8f88-5f1f2213d737"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 5\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "vCfRYHE_ENPo",
        "outputId": "713c4631-a78d-4200-acf1-d1ac2308776d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=6.141793568929036, metrics={'train_runtime': 2.4045, 'train_samples_per_second': 6.238, 'train_steps_per_second': 2.495, 'total_flos': 245810626560.0, 'train_loss': 6.141793568929036, 'epoch': 3.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5. Fine-Tune the Model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset2,  # Ensure pad token is properly handled\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xKIaLHd6FFmt"
      },
      "outputs": [],
      "source": [
        "# 6. Perform Inference\n",
        "# model.eval()\n",
        "# input_text = \"AI is transforming the \"\n",
        "# inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "# output = model.generate(inputs[\"input_ids\"],\n",
        "#                         max_length=50,\n",
        "#                         num_return_sequences=1,\n",
        "#                         temperature=0.7,  # Controls randomness in output; lower = more deterministic\n",
        "#                         top_p=0.9,  # Nucleus sampling: restricts the next token to a top-probability set\n",
        "#                         no_repeat_ngram_size=2,  # Prevents repetition of n-grams\n",
        "#                         do_sample=True  # Enables sampling instead of greedy decoding (which is deterministic)\n",
        "#                         )\n",
        "# print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK2FPsopFqtK"
      },
      "source": [
        "To resolve the warning, you need to explicitly provide an attention_mask along with the input IDs during inference. Additionally, you should set pad_token_id explicitly for text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCc533WdFoV3",
        "outputId": "2a8f7cf7-6536-4a95-d0b2-7b42c6207183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Responses:\n",
            "Option 1: AI in oncology.\n",
            "\n",
            "The following is a quick summary of the information we have gathered on the subject. We will not be making any statements about the efficacy of drugs. Instead, we will be providing you with information about what's in your system. As always, our goal is to provide a comprehensive overview of this topic, so you can make informed decisions. However, you may wish to consult with your healthcare provider before taking any medication.\n",
            "\n",
            "Option 2: AI in oncology and neurophysiology.\n",
            "\n",
            "\"The neurobiological basis of Parkinson's disease is poorly understood and largely unknown. We are now able to investigate the mechanisms of neurodegeneration in the mouse model of the disease and to establish the role of dopamine and norepinephrine in controlling neuropathology,\" says Dr. John M. A. Bickley, PhD, from the Department of Neurobiology and Neurosciences at the University of Oxford and the lead author\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 6. Perform Inference\n",
        "model.eval()\n",
        "input_text = \"AI in oncology\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "\n",
        "# Explicitly set pad_token_id\n",
        "# Set parameters to influence generation quality\n",
        "output = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],  # Pass attention mask\n",
        "    max_length=100,  # Limit the length to avoid overly verbose answers\n",
        "    num_return_sequences=2,  # Generate multiple options for comparison\n",
        "    pad_token_id=tokenizer.eos_token_id,  # Ensure pad token is properly handled\n",
        "    temperature=0.7,  # Adjust temperature for controlled randomness (lower means less randomness)\n",
        "    top_p=0.9,  # Increase top_p for more diverse but focused output\n",
        "    top_k=50,  # Top-k sampling limits the pool of options for better quality\n",
        "    no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
        "    do_sample=True,  # Enable sampling for more creative responses\n",
        ")\n",
        "\n",
        "# Decode tokens and print the output\n",
        "print(\"Generated Responses:\")\n",
        "for i, generated_sequence in enumerate(output):\n",
        "    print(f\"Option {i+1}: {tokenizer.decode(generated_sequence, skip_special_tokens=True)}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
