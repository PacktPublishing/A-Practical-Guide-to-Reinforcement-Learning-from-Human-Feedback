{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ZufpwnkMg7",
        "outputId": "36a773bf-4cd9-4041-cebd-701b5f8dc5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.12.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.12.2-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 trl-0.12.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0Ii9yLyCMrp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "c74772f095b046f6a282871b47344bd2",
            "f7692554ac5c456f92777c685bd47696",
            "cd8e915304eb43369392d7e8e9fdb6fd",
            "2a94ab35cf1c4e799718faffa8cde122",
            "3ca80f4fa4dd401cb800a1c51cec83a5",
            "57038a1214594945bba4b0beeb3835e0",
            "3095e01f47124c39a013ccb2fb07972e",
            "ebc0f494bd13484ebe66e5383a5b9366",
            "f318e8a95bc049cbab6b427a661cc14c",
            "aeeee432d6e8443880dbef9430e0f333",
            "6da8fa2189af4e5ca6e4c4c8b477a5c6",
            "9baa0317e88640ac82e466ccba975a65",
            "21601d88a26f4368859dffd8a4f33e1a",
            "b97edbd18c954e4b84674b630e290f0c",
            "5e0e5cc1368e45d5a39a2f91ba160d0e",
            "0030931b71524038be87ade3e95f5c83",
            "1eef283ed7c54484982e4b5c3853474d",
            "fe54a538936f4c2aab80f6897b2bb919",
            "8c84697b6e674e55b391a825e7373c4b",
            "5bc9325b8f264747948d317c861eddc8",
            "8b3d1b57624c4f0897194c0b3fa39264",
            "2adce25e2f5a4ef28e5ddebb97efc014",
            "3ba1c88580b84caabca988b9557e16ea",
            "d7fd16a2262a4a999fcb14ec77e9bde3",
            "0c6304d5c0774066bc97a21005135f20",
            "751f91fb5fa74f208dbc8bd0f5b66462",
            "66658cf18d6e437db92f6752c3185db4",
            "7827d5fc3bf74445a1d07e4ccbf51654",
            "c1b3014750ff4eac847dbdfdbd17e383",
            "18e97ce1681d4f0db3d59deb153200de",
            "5194e1351a5749688846eedc38539fd7",
            "be064989442348d8bcd7e1a29d99069c",
            "4b83d95449bc432cbff78441e986336c",
            "bbe31f8fd5c042799ce89fd0f929c431",
            "a50282d5e34b402681781a0db9ebbbcc",
            "2879100e7da94504a15c33fbabbacc53",
            "b73c8ab6b0e04567a42ff43c9dfae842",
            "37302c384e1d4f35ab1c7678d85fd996",
            "d254f1aacde14c619d852c8e517ef0f9",
            "1960e3fcf1a049c3b60507315b2f7652",
            "172583433edb4a86889b2d77c6d07145",
            "d558666c02a64671b5220da48349e519",
            "086dca41041d4ef0a489d0b434d05a83",
            "07c55528a0aa469fb98c3f03d26b52e7",
            "06b356dcc2d74c6193e45b8496686ee7",
            "a566485f4567439383de1c2e01235fc2",
            "f347d40dfc2a449a97676e3103612e96",
            "fe7170abc94945daa5b3bea00906453f",
            "b054c9c0fc764c23b6913c9430470c4b",
            "2634f798ed164e578891dc74c9130ce2",
            "8111d0b59acc463781f0b790a22b0c43",
            "00990b0d20984073a51d4a81187d1442",
            "cf8d0b0c2f344ea29f5c1470c32806b3",
            "fc7056f51d5b4df385157df65bc6b7b2",
            "81726d1c85b34a2a813de6b152962c76",
            "b1ea877056b447dfa117ec7fd8e6fd62",
            "a8dfc23b957348e0b30fe7784f90a99b",
            "54e7d29279284aceb3b52ef7ef407e44",
            "4c626c64935b4269b2c2c009e4090a30",
            "b9635844f99848dcabedfeeeca3eea06",
            "b95084caa781476d8c97983dbefac806",
            "557656dc0436444d92430d83f97d776b",
            "0202989af0994aa3b51713159975ae68",
            "8be61f5ac7964daba01f08ffa4153ea8",
            "be72686bbd674311b27f2bb897936675",
            "46aa1fe6c1ca4b3b8f3a168a95d7e487",
            "ee49cb1a9a334d2e854315145db164a4",
            "b8092fbc0d954aa6b3e067e3be5acb82",
            "efe463e2c0374824903f0e8cad0ddd95",
            "ce769307b77246159c99e0d68a924204",
            "cc86b9f77c5f4de5a21f33c9d4e766bd",
            "6e7997a5e9d84fc0b2669c915b3964fb",
            "3374042ff3414ab6b2e10e424a31e1bc",
            "79abc21b2db6471dbed31ffbdba0296c",
            "ff1cae2faed646bbb9e740302aa03c40",
            "02fb7d3602a84a80aea4159e37fbb30b",
            "c21e80dfa6d7424cb87978f41a9fbbfe"
          ]
        },
        "id": "hLUQyX3nCYgz",
        "outputId": "a61cfac7-026a-4afb-fe20-1bf8dc800b0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c74772f095b046f6a282871b47344bd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9baa0317e88640ac82e466ccba975a65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ba1c88580b84caabca988b9557e16ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbe31f8fd5c042799ce89fd0f929c431",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06b356dcc2d74c6193e45b8496686ee7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1ea877056b447dfa117ec7fd8e6fd62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee49cb1a9a334d2e854315145db164a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 1. Load Pre-trained Model and Tokenizer\n",
        "model_name = \"gpt2\"  # Can be any Hugging Face causal language model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Set a padding token for GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnz2fNHUCh6g"
      },
      "outputs": [],
      "source": [
        "# 2. Define PEFT Configuration (LoRA)\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,          # Rank of LoRA matrices\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\"],  # Adjusted for GPT-2\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAJ4ZFo89TXa",
        "outputId": "84d0b458-b5da-46c7-947c-b4eed465983e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model without peft\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUwIBYr-EWsj",
        "outputId": "5f40b2da-6416-4ba6-ade1-916c673f250d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2SdpaAttention(\n",
              "              (c_attn): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=2304, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (c_proj): Conv1D(nf=768, nx=768)\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=3072)\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DSNbChYDeNg"
      },
      "outputs": [],
      "source": [
        "# 3. Prepare Custom Dataset\n",
        "data_1 =  {\n",
        "    \"text\": [\n",
        "        \"Artificial intelligence is revolutionizing many industries by automating tasks and improving decision-making.\",\n",
        "        \"Machine learning models are used in applications like predictive analytics and natural language processing.\",\n",
        "        \"AI can assist in various sectors such as healthcare, finance, and transportation by analyzing large datasets.\",\n",
        "        \"Deep learning techniques are powering advancements in computer vision, enabling machines to interpret images and video.\",\n",
        "        \"AI technologies like robotics are being deployed to perform tasks that were once considered too complex for machines.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset_1 = Dataset.from_dict(data_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT-wZDEly6xz"
      },
      "outputs": [],
      "source": [
        "data_2 = {\n",
        "    \"text\": [\n",
        "        \"AI is transforming oncology by helping in the early detection of cancer through advanced imaging techniques.\",\n",
        "        \"Machine learning algorithms are increasingly used to analyze tumor biopsies and predict cancer progression.\",\n",
        "        \"In precision oncology, AI is utilized to create personalized treatment plans based on a patient's genetic profile.\",\n",
        "        \"AI-driven systems are improving the accuracy of radiology by identifying tumors in medical scans like CT and MRI images.\",\n",
        "        \"Artificial intelligence is accelerating drug discovery, identifying potential cancer therapies faster than traditional methods.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset_2 = Dataset.from_dict(data_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fyVsJpAzATp"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    tokenized = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Add labels for loss computation\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iW9bhnIEDVq"
      },
      "outputs": [],
      "source": [
        "# 4. Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=3,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"no\",  # Skip saving for simplicity\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    report_to= 'none',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "b9f93a726b8940b38489b78ce5ca8a68",
            "77d6ef3fe50a4f259a5b5876116c7a6d",
            "1394504ef1f34207a72efb540d0f4d2c",
            "54d3c190edd247309dc8348b6f291abf",
            "8f16a0af81cd41b8a40624148da2b131",
            "31490a8e5da9431eb51507f78dc931a1",
            "87a4584868754cf09a595f1cc02656a3",
            "fa8969a5f74d4675b9dd8dffa989f452",
            "38e3c8358ddd47728e08bd2b5118c48b",
            "28514371412d4b629099b13d1707a778",
            "4bc96b227bcc407bb194b3ad542cbc28",
            "c594ea9691c04fccb929f22a51c73b75",
            "7f1a33d6bb3a42759cd3e469123edfc2",
            "a7b28d6cb998486db98fc81598c2bf15",
            "c5d94159273c42f7acffd603a8396d1a",
            "07134e07dd6147feb3507f23c4b09e3e",
            "450a093a8e664fa7876d76c758a4da86",
            "5b16b00ab9b34710bb42451d0aefd394",
            "6822212904554871920fadfbc01bb7f9",
            "bd352688d5da4f949b16964e49460403",
            "340d1013a65d4640beeef2853bcc0946",
            "90ccdc8b28f3405285057752165540cf"
          ]
        },
        "id": "wCkT5sl1TQmD",
        "outputId": "b0bb970e-89d5-4c06-b34e-d2f85b378e77"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9f93a726b8940b38489b78ce5ca8a68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c594ea9691c04fccb929f22a51c73b75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset1 = dataset_1.map(tokenize_function, batched=True)\n",
        "tokenized_dataset2 = dataset_2.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMFoUqKTfvy",
        "outputId": "2bf464f7-943e-477e-9857-cd5ee9aa6e00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 5\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "vCfRYHE_ENPo",
        "outputId": "0b57961e-3f66-4c7c-dec3-1b25a554f0c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=4.512209574381511, metrics={'train_runtime': 1.7157, 'train_samples_per_second': 8.743, 'train_steps_per_second': 1.749, 'total_flos': 245810626560.0, 'train_loss': 4.512209574381511, 'epoch': 3.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5. Fine-Tune the Model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset2,  # Ensure pad token is properly handled\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKIaLHd6FFmt"
      },
      "outputs": [],
      "source": [
        "# 6. Perform Inference\n",
        "model.eval()\n",
        "input_text = \"AI is transforming the \"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(inputs[\"input_ids\"],\n",
        "                        max_length=50,\n",
        "                        num_return_sequences=1,\n",
        "                        temperature=0.7,  # Controls randomness in output; lower = more deterministic\n",
        "                        top_p=0.9,  # Nucleus sampling: restricts the next token to a top-probability set\n",
        "                        no_repeat_ngram_size=2,  # Prevents repetition of n-grams\n",
        "                        do_sample=True  # Enables sampling instead of greedy decoding (which is deterministic)\n",
        "                        )\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK2FPsopFqtK"
      },
      "source": [
        "T resolve the warning, you need to explicitly provide an attention_mask along with the input IDs during inference. Additionally, you should set pad_token_id explicitly for text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCc533WdFoV3",
        "outputId": "f01b0f07-dabb-40e7-8459-db6dd9b62b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Responses:\n",
            "Option 1: AI in oncology and applied biology. The research was funded by the National Institute of Mental Health (NIH) and the American College of Radiology (ACR).\n",
            "\n",
            "About the NIH\n",
            " (R01 DK1134): NIH, the nation's medical research agency, includes 27 Institutes and Centers and is a component of the U.S. Department of Health and Human Services. NIH is the primary federal agency conducting and supporting basic, clinical, and translational medical and medical imaging research\n",
            "\n",
            "Option 2: AI in oncology, we have shown that the brain is not a part of the human brain and that it is capable of generating and maintaining information. This knowledge is useful to the study of mental health, but it does not mean that we should ignore it. We can learn from it and use it to help others.\n",
            "\n",
            "The brain has many functions. The most important are learning to reason, thinking, and writing. Most of these functions are not found in the prefrontal cortex, which is\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 6. Perform Inference\n",
        "model.eval()\n",
        "input_text = \"AI in oncology\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "\n",
        "# Explicitly set pad_token_id\n",
        "# Set parameters to influence generation quality\n",
        "output = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],  # Pass attention mask\n",
        "    max_length=100,  # Limit the length to avoid overly verbose answers\n",
        "    num_return_sequences=2,  # Generate multiple options for comparison\n",
        "    pad_token_id=tokenizer.eos_token_id,  # Ensure pad token is properly handled\n",
        "    temperature=0.7,  # Adjust temperature for controlled randomness (lower means less randomness)\n",
        "    top_p=0.9,  # Increase top_p for more diverse but focused output\n",
        "    top_k=50,  # Top-k sampling limits the pool of options for better quality\n",
        "    no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
        "    do_sample=True,  # Enable sampling for more creative responses\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "print(\"Generated Responses:\")\n",
        "for i, generated_sequence in enumerate(output):\n",
        "    print(f\"Option {i+1}: {tokenizer.decode(generated_sequence, skip_special_tokens=True)}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
